{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43be1902",
   "metadata": {},
   "source": [
    "### Comentários do desenvolvimento do projeto "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dced5a38",
   "metadata": {},
   "source": [
    "### aiko:\n",
    "\n",
    "Até agora a gente esbarrou em alguns problemas, mas o principal até então foi o\n",
    "da precisão do dataset, ao tentar analisar o resultado final conseguimos notar diversas divergências de classificação.\n",
    "\n",
    "na minha cabeça a solução chegou da seguinte forma:\n",
    "\n",
    "1- Criar versões paralelas de cada dataset, para analisar antes e depois do tratamento os dados classificados como contendo hate_speech.\n",
    "estou fazendo isso por partes, dentro da pasta \"dataset_2\" existem 3 arquivos principais:\n",
    "\n",
    "### DATASET_2 \n",
    "\n",
    "1.1 - \"olid_apenas_hate_bruto\" que nada mais é do que a tentativa de avaliar manualmente a acurácia de classificação dos dados dentro do dataset, a gente dropou todas as colunas do dataset que não continham nenhuma ofensa direcionada a grupos (justamente o nosso objetivo identificar). Dentro do arquivo, há em todas as linhas (de acordo com a contrução original do dataset) no mínimo um caso de ataque a grupo, sendo True qualquer uma das colunas : \"racism,sexism,xenophobia,lgbtqphobia,religious_intolerance\".\n",
    "Vale lembrar que esse arquivo contém todos os dados do \"text\" sem nenhum tipo de tratamento. Porém, ao fazer esse recorte no dataset, deixando apenas os casos onde o ataque era direcionado a um grupo e pelo menos uma das colunas citadas era verdadeira, nosso dataset teve uma redução circustancial de aproximadamente 5000 labels para 174.\n",
    "\n",
    "1.2 - \"olid_apenas_hate_tratado\" é um arquivo onde separei as linhas do dataset previamente tratado. De que forma? basicamente se qualquer uma das colunas: \"racism,sexism,xenophobia,lgbtqphobia,religious_intolerance\" do dataset já tratado fossem verdadeira, a linha seria mantida, caso contrário excluida. Depois disso criei uma nova coluna binária para simplificar a avaliação manual. Ela vai ser '1' quando qualquer uma das colunas acima for dada como verdadeira, e '0' quando não. Porém nesse arquivo estão apenas os casos de '1', uma vez que o erro mais recorrente era uma classificação equivocada de 0 para 1 e não de 1 para 0.\n",
    "\n",
    "1.3 -\"olid_tratado\" é o arquivo \"final\", o dataset completo tratado, porém com as mesma incongruências citadas. Onde uma determinada frase que supostamente deveria ser classificada como 0 está sendo classificada como 1. Porém o arquivo em questão levou em conta o mesmo princípio de criação do \"olid_apenas_hate\", contendo a seguinte lógica: Todas as nuânces do \"text\" já foram tratadas (desconsiderando temporariamente a stemming), o arquivo armazenou todos os casos, tanto de \"1\" quanto de \"0\", sendo \"1\" quando uma das colunas anteriormente citadas fosse verdadeira e \"0\" o oposto disso. \n",
    "\n",
    "2 - Agora vou avaliar manualmente a precisão classificatória de ambos os arquivos, para conseguir entender melhor qual é o problema que está gerando essas incongruências de classificação. Porque, o problema pode ser de duas origens: \n",
    "\n",
    "2.1 - O dataset foi classificado de uma forma que não se adequa ao que nós precisamos\n",
    "\n",
    "2.2 - Ao ler o dataset tratado, a ambiguidade e subjetividade tomam lugar e interferem na nossa interpretação do texto, consequentemente interferindo na nossa escolha de classificação.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
