{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43be1902",
   "metadata": {},
   "source": [
    "### Comentários do desenvolvimento do projeto "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dced5a38",
   "metadata": {},
   "source": [
    "### aiko:\n",
    "\n",
    "até agora a gente esbarrou em alguns problemas, mas o principal até então foi o\n",
    "da precisão do dataset, ao tentar analisar o resultado final conseguimos notar diversas divergências de classificação.\n",
    "\n",
    "na minha cabeça a solução chegou da seguinte forma:\n",
    "\n",
    "1- Criar versões paralelas de cada dataset, para analisar antes e depois do tratamento os dados classificados como contendo hate_speech.\n",
    "estou fazendo isso por partes, dentro da pasta \"dataset_2\" existem 3 arquivos principais:\n",
    "\n",
    "\n",
    "1.1 - \"olid_apenas_hate_bruto\" que nada mais é do que a tentativa de avaliar manualmente a acurácia de classificação dos dados dentro do dataset, a gente dropou todas as colunas do dataset que não continham nenhuma ofensa direcionada a grupos (justamente o nosso objetivo identificar). Dentro do arquivo, há em todas as linhas (de acordo com a contrução original do dataset) no mínimo um caso de ataque a grupo, sendo True qualquer uma das colunas : \"racism,sexism,xenophobia,lgbtqphobia,religious_intolerance\".\n",
    "Vale lembrar que esse arquivo contém todos os dados do \"text\" sem nenhum tipo de tratamento.\n",
    "\n",
    "1.2 - \"olid_apenas_hate_tratado\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
